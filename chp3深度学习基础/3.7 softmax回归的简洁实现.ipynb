{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-algebra",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.6 softmax回归的简洁实现\n",
    "# start coding at 01-23 13:33 on Mac\n",
    "# target1: 加入一个FlattenLayer把（batch_size, 1, 28, 28）的小批量图片转换成（batch_size, 784）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "forbidden-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "import d2lzh_pytorch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-pride",
   "metadata": {},
   "source": [
    "# 1. 获取和读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "labeled-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 仍然使用FMnist数据集和上一节中使用的批量大小\n",
    "batch_size = 356\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-prime",
   "metadata": {},
   "source": [
    "# 2.定义模型和初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "upset-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax回归的输出层是一个全连接层，所以我们用一个线性模块就可以了。\n",
    "# 因为前面我们数据返回的每个batch样本x的形状为(batch, 1, 28, 28), 所以我们要先用view()将x的形状转换成(batch_size, 784)才送入全连接层。\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "\n",
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "    def forward(self, x): # x shape: (batch, *, *, ...)\n",
    "        return x.view(x.shape[0], -1)\n",
    "    \n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "net = nn.Sequential(\n",
    "    OrderedDict([\n",
    "        ('flatten', FlattenLayer()),\n",
    "        ('linear', nn.Linear(num_inputs, num_outputs))\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sought-september",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init.normal_(net.linear.weight, mean=0, std=0.01)\n",
    "init.constant_(net.linear.bias, val=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-court",
   "metadata": {},
   "source": [
    "# 3. softmax和交叉熵损失函数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eleven-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-seeker",
   "metadata": {},
   "source": [
    "# 4.定义优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "young-chick",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-producer",
   "metadata": {},
   "source": [
    "# 5.训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "integrated-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params=None, lr=None, optimizer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y).sum()\n",
    "            \n",
    "            # 梯度清零\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            \n",
    "            # 根据损失函数求梯度\n",
    "            l.backward()\n",
    "            \n",
    "            # 让优化器去优化参数\n",
    "            if optimizer is None:\n",
    "                d2l.sgd(params, lr, batch_size)\n",
    "            else:\n",
    "                optimizer.setp()\n",
    "                \n",
    "            # 全局损失\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).float().sum().item()\n",
    "            n += y.shape[0]\n",
    "        \n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print(\"epoch %d, loss %.3f, train acc %.3f test acc %.3f\"\n",
    "             %(epoch+1, train_l_sum, train_acc_sum/n, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "reported-allocation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0024, train acc 0.732, test acc 0.772\n",
      "epoch 2, loss 0.0017, train acc 0.806, test acc 0.801\n",
      "epoch 3, loss 0.0016, train acc 0.820, test acc 0.812\n",
      "epoch 4, loss 0.0015, train acc 0.827, test acc 0.810\n",
      "epoch 5, loss 0.0014, train acc 0.831, test acc 0.818\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finished at 14:00"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Voygern",
   "language": "python",
   "name": "voygern"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
