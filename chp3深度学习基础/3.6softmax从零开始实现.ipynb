{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.5 图像分类数据集\n",
    "# start coding at 01-23 10:30 on Mac\n",
    "# target1: softmax的结构（输入和输出），矩阵表示\n",
    "# target2: 实现softmax运算 X.sum(dim=0, keepdim=True) 的用法\n",
    "# target3: gather函数的用法：取下标的值\n",
    "# target4: 理解每次的输出epoch 1, loss 28395.907, train acc 0.840, test acc 0.830\n",
    "#                      第几次？  全局loss         在训练集上的精度   在测试集上的精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "atlantic-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\") # 为了导入上层目录的d2lzh_pytorch\n",
    "import d2lzh_pytorch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-seattle",
   "metadata": {},
   "source": [
    "# 1. 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "global-nevada",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-proposal",
   "metadata": {},
   "source": [
    "# 2. 初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cordless-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "W = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_outputs)), dtype=torch.float)\n",
    "b = torch.zeros(num_outputs, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-anime",
   "metadata": {},
   "source": [
    "## 需要模型梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "correct-blair",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.requires_grad_(requires_grad=True)\n",
    "b.requires_grad_(requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-implement",
   "metadata": {},
   "source": [
    "# 3. 实现softmax运算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-jones",
   "metadata": {},
   "source": [
    "在介绍如何定义softmax回归之前，**我们先描述一下对如何对多维Tensor按维度操作**。\n",
    "在下面的例子中，给定一个Tensor矩阵X。我们可以只对其中`同一列（dim=0）`或`同一行（dim=1）`的元素求和，并在结果中保留行和列这两个维度（keepdim=True）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "suspected-looking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 7, 9]])\n",
      "tensor([[ 6],\n",
      "        [15]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([\n",
    "    [1,2,3],\n",
    "    [4,5,6]])\n",
    "print(X.sum(dim=0, keepdim=True))\n",
    "print(X.sum(dim=1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "turned-messenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = X.exp()\n",
    "    partition = X_exp.sum(dim=1, keepdim=True)\n",
    "    return X_exp / partition # 这里用了广播机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-asian",
   "metadata": {},
   "source": [
    "# 4.定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fatty-pizza",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return softmax(torch.mm(X.view(-1, num_inputs), W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-brief",
   "metadata": {},
   "source": [
    "# 5.定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "terminal-prior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000],\n",
       "        [0.5000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = torch.tensor([\n",
    "[0.1, 0.3, 0.6],\n",
    "[0.3, 0.2, 0.5]])\n",
    "\n",
    "# 变量y是这2个样本的标签类别\n",
    "y = torch.LongTensor([0, 2])\n",
    "\n",
    "y_hat.gather(1, y.view(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "graduate-russian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return - torch.log(y_hat.gather(1, y.view(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-florist",
   "metadata": {},
   "source": [
    "# 6.计算分类准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mysterious-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 为了演示准确率的计算，下面定义准确率accuracy函数。\n",
    "# 其中y_hat.argmax(dim=1)返回矩阵y_hat每行中最大元素的索引，且返回结果与变量y形状相同。\n",
    "# 相等条件判断式(y_hat.argmax(dim=1) == y)是一个类型为ByteTensor的Tensor，\n",
    "# 我们用float()将其转换为值为0（相等为假）或1（相等为真）的浮点型Tensor。\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    return (y_hat.argmax(dim=1) == y).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sought-research",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "### 可以看到，第一个样本预测类别为2（该行最大元素0.6在本行的索引为2），与真实标签0不一致；\n",
    "# 第二个样本预测类别为2（该行最大元素0.5在本行的索引为2），与真实标签2一致。\n",
    "# 因此，这两个样本上的分类准确率为0.5。\n",
    "\n",
    "print(accuracy(y_hat, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bacterial-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "        n += y.shape[0]\n",
    "        \n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "given-proportion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1065\n"
     ]
    }
   ],
   "source": [
    "### 瞎猜\n",
    "# 因为我们随机初始化了模型net，所以这个随机模型的准确率应该接近于类别个数10的倒数即0.1。\n",
    "print(evaluate_accuracy(test_iter, net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "empirical-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr = 5, 0.1\n",
    "\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params=None, lr=None, optimizer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y).sum()\n",
    "            \n",
    "            # 梯度清零\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            \n",
    "            # 根据损失函数求梯度\n",
    "            l.backward()\n",
    "            \n",
    "            # 让优化器去优化参数\n",
    "            if optimizer is None:\n",
    "                d2l.sgd(params, lr, batch_size)\n",
    "            else:\n",
    "                optimizer.setp()\n",
    "                \n",
    "            # 全局损失\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).float().sum().item()\n",
    "            n += y.shape[0]\n",
    "        \n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print(\"epoch %d, loss %.3f, train acc %.3f test acc %.3f\"\n",
    "             %(epoch+1, train_l_sum, train_acc_sum/n, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "color-uncertainty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 28395.907, train acc 0.840 test acc 0.830\n",
      "epoch 2, loss 27908.211, train acc 0.842 test acc 0.831\n",
      "epoch 3, loss 27514.323, train acc 0.845 test acc 0.834\n",
      "epoch 4, loss 27125.277, train acc 0.846 test acc 0.833\n",
      "epoch 5, loss 26824.521, train acc 0.848 test acc 0.832\n"
     ]
    }
   ],
   "source": [
    "train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finished at 01-23 13;25"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Voygern",
   "language": "python",
   "name": "voygern"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
